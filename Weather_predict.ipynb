{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram') # Để trực quan hóa pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Request Link](https://globalweather.tamu.edu/request/view/36316)\\\n",
    "[Direct download](https://globalweather.tamu.edu/data/cfsr/36316_2020-12-31-02-56-09.zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('weatherdata-1611081.csv', index_col=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_range = 5000\n",
    "for col in [ 'Max Temperature', 'Min Temperature', 'Precipitation', 'Wind', 'Relative Humidity','Solar']:\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.scatter(pd.RangeIndex(start=0, stop=display_range, step=1), df[col].head(display_range))\n",
    "    plt.title(label=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_temp = (df['Max Temperature'] + df['Min Temperature'])/2\n",
    "mean_temp.head(10000).reset_index().plot(x='index', y=0, kind = 'scatter', figsize=[20,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.insert(loc=3, column='Mean Temperature', value=mean_temp)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Derive n<sup>th</sup> day features pipeline (and drop some unnecessary columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColAdderDropper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, nth_day_features=0):\n",
    "        self.nth_day_features = nth_day_features\n",
    "    def fit(self, X_df, y=None):\n",
    "        return self\n",
    "    def derive_nth_day_feature(self, X, feature, N):\n",
    "        rows = X.shape[0]\n",
    "        nth_prior_measurements = [None]*N + [X[feature][i-N] for i in range(N, rows)]\n",
    "        col_name = \"{} {}\".format(feature, N)\n",
    "        X[col_name] = nth_prior_measurements\n",
    "    def transform(self, X, y=None):\n",
    "        X_cl = X.copy()\n",
    "        drop_col = X_cl.columns\n",
    "        X_cl.drop(['Longitude','Latitude','Elevation','Date', 'Mean Temperature'], axis=1, errors='ignore', inplace=True)\n",
    "        for feature in X_cl.columns:\n",
    "            for N in range(1, self.nth_day_features + 1):\n",
    "                self.derive_nth_day_feature(X_cl, feature, N)\n",
    "        X_cl.drop(drop_col, axis=1, errors='ignore', inplace=True)\n",
    "        X_cl.fillna(method='bfill', inplace=True)\n",
    "        return X_cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "coladderdropper = make_pipeline(ColAdderDropper(nth_day_features=3))\n",
    "preprocessed_train_X = coladderdropper.transform(df)\n",
    "preprocessed_train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X = coladderdropper.transform(df)\n",
    "y = df['Mean Temperature']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "regressor = LinearRegression()\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# make a prediction set using the test set\n",
    "prediction = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the prediction accuracy of the model\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "print(\"The Explained Variance: %f\" % regressor.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X.head(5))\n",
    "display(y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tách tập huấn luyện và tập validation theo tỉ lệ 80:20\n",
    "\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X_train, y_train, test_size=0.25, random_state=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_df.shape)\n",
    "print(y_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(StandardScaler(), \n",
    "                         MLPRegressor(hidden_layer_sizes=(8,,8), activation='tanh', solver='adam', max_iter=5000))\n",
    "train_errs = []\n",
    "val_errs = []\n",
    "alphas = [0.01, 0.15, 1, 3,5,6,10]\n",
    "best_val_err = float('inf'); best_alpha = None;\n",
    "                         \n",
    "for alpha in alphas:\n",
    "    pipeline.set_params(mlpregressor__alpha = alpha)\n",
    "    pipeline.fit(X_train_df, y_train_df)\n",
    "    pipeline.predict(X_val_df)\n",
    "    train_err = (1 - pipeline.score(X_train_df, y_train_df))*100\n",
    "    val_err = (1 - pipeline.score(X_val_df, y_val_df))*100\n",
    "    if val_err < best_val_err:\n",
    "        best_val_err = val_err\n",
    "        best_alpha = alpha\n",
    "    train_errs.append(train_err)\n",
    "    val_errs.append(val_err)\n",
    "'Done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_errs,color=\"red\",)\n",
    "plt.plot(val_errs,color=\"yellow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errs_df = pd.DataFrame(data=np.array(train_errs).reshape(len(alphas), -1),\n",
    "                             index=alphas)\n",
    "val_errs_df = pd.DataFrame(data=np.array(val_errs).reshape(len(alphas), -1), \n",
    "                           index=alphas)\n",
    "min_err = min(min(train_errs), min(val_errs))\n",
    "max_err = max(max(train_errs), max(val_errs))\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(train_errs_df, vmin=min_err, vmax=max_err, square=True, annot=True, \n",
    "            cbar=False, fmt='.1f', cmap='Reds')\n",
    "plt.title('train errors'); plt.xlabel('num_top_titles'); plt.ylabel('alpha')\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(val_errs_df, vmin=min_err, vmax=max_err, square=True, annot=True, \n",
    "            cbar=False, fmt='.1f', cmap='Reds')\n",
    "plt.title('validation errors'); plt.xlabel('num_top_titles'); plt.ylabel('alpha');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.set_params(mlpregressor__alpha = best_alpha)\n",
    "pipeline.fit(X_train, y_train)\n",
    "pipeline.predict(X_test)\n",
    "test_score=pipeline.score(X_test,y_test)\n",
    "print(\"The Explained Variance: %f\" % test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
